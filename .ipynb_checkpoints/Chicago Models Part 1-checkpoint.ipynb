{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "829d183a",
   "metadata": {},
   "source": [
    "# Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f6d01b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set\n",
    "import warnings\n",
    "import re\n",
    "from pandas.io import gbq\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "import xgboost\n",
    "import pickle\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "from scipy import sparse\n",
    "#Custom Python Module with functions specifically for this project\n",
    "import ChicagoDataCleaningFunctions as cd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c571034",
   "metadata": {},
   "source": [
    "# Get the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b8d7c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read the data in from Google BigQuery\n",
    "chicago_data = \"\"\"\n",
    "                    SELECT unique_key, date, primary_type, location_description, \n",
    "                            arrest, domestic, community_area, year\n",
    "                    FROM `gdac-327115.Chicago.chicago2`\n",
    "                    WHERE year >= 2011\n",
    "               \"\"\"\n",
    "chicago_data = gbq.read_gbq(chicago_data, project_id=\"gdac-327115\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e9c168e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in an Excel file with a one to one mapping between Chicago community areas and districts\n",
    "chicago_districts = pd.read_excel(\"ChicagoCommunityAreas.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95d68d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data type can't be joined on an int\n",
    "chicago_districts.community_area = chicago_districts[\"community_area\"].astype(\"string\")\n",
    "chicago_data.community_area = chicago_data[\"community_area\"].astype(\"string\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30d971d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Outer join the two data sets\n",
    "chicago = chicago_data.merge(chicago_districts, how=\"outer\", left_on=\"community_area\", right_on=\"community_area\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62f75180",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop the community area variable since we have a community name variable\n",
    "chicago.drop(\"community_area\", axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52ccf40",
   "metadata": {},
   "source": [
    "# Split the Data into Training and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a791c216",
   "metadata": {},
   "outputs": [],
   "source": [
    "chicago_train = chicago.loc[chicago[\"year\"] != 2021]\n",
    "chicago_test = chicago.loc[chicago[\"year\"] == 2021]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591440bf",
   "metadata": {},
   "source": [
    "# Clean the Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0fd42c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning Started...\n",
      "\n",
      "Successfully Cleaned Primary Type\n",
      "Successfully Imputed Location\n",
      "Successfully Cleaned Location\n",
      "Successfully Added Month Column\n",
      "Successfully Added Hour Column\n",
      "Successfully Cleaned Community\n",
      "\n",
      "Data Set Successfully Cleaned!\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stdout\n",
    "cd.chicago_data_cleaner(chicago_train, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40aa46f",
   "metadata": {},
   "source": [
    "# Prepare the Data for Modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa5ff5b",
   "metadata": {},
   "source": [
    "Since all of our variables are categorical, we'll need to one-hot-encode all of them. Also, note that we will not be using year as a feature. This is because the final test set will only use data from 2021. Future considerations could treat this problem as a time series problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4197fb13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Month</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>16008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>12852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>15709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>15279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>17493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>18468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>18898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>18110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>18632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>5507</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Month  Count\n",
       "5      1  16008\n",
       "8      2  12852\n",
       "6      3  15709\n",
       "7      4  15279\n",
       "4      5  17493\n",
       "2      6  18468\n",
       "0      7  18898\n",
       "3      8  18110\n",
       "1      9  18632\n",
       "9     10   5507"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check if the test set contains data from the full year\n",
    "chicago_test.loc[:, \"date\"].dt.month. \\\n",
    "                                value_counts(). \\\n",
    "                                reset_index(). \\\n",
    "                                rename(columns={\"index\":\"Month\", \"date\":\"Count\"}). \\\n",
    "                                sort_values(by = \"Month\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb8af94",
   "metadata": {},
   "source": [
    "We are reminded that our final test set does not include the final two months of the year. Thus, when we transform the Month variable we'll have to drop the \"11\" and \"12\" columns to ensure that our training data matches up with the test data. We'll do this in a function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38e04f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_chicago_train(df, attribs):\n",
    "    \"\"\"\n",
    "    This function is just a convenient wrapper around the ColumnTransformer method for OneHotEncoding categorical features\n",
    "    specific to the training data\n",
    "    \n",
    "    df: DataFrame\n",
    "    attribs: Columns specified to be transformed. Expected data structure is a list\n",
    "    \n",
    "    returns: X(Sparse Matrix): y(Series)\n",
    "    \"\"\"\n",
    "    #Get a separate list for the time variables\n",
    "    date_attribs = [attribs.pop(attribs.index(\"Month\")), attribs.pop(attribs.index(\"Hour\"))]\n",
    "    \n",
    "    #One hot encode the variables\n",
    "    cat_encoder = OneHotEncoder()\n",
    "    X_sub = cat_encoder.fit_transform(df[attribs])\n",
    "    \n",
    "    #One hot encode the time variables but produce a dense matrix to drop the 11th and 12th values from month\n",
    "    cat_encoder = OneHotEncoder(sparse = False)\n",
    "    X_date = sparse.csr_matrix(np.delete(cat_encoder.fit_transform(df[date_attribs]), [10, 11], axis = 1))\n",
    "    \n",
    "    #Stack the two back together\n",
    "    X = sparse.hstack((X_sub, X_date))\n",
    "    y = (df[\"arrest\"] == True).astype(np.int)\n",
    "    \n",
    "    return X, y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b32eccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_chicago_test(df, attribs):\n",
    "    \"\"\"\n",
    "    This function is just a convenient wrapper around the ColumnTransformer method for OneHotEncoding categorical features\n",
    "    specific to the test data\n",
    "    \n",
    "    df: DataFrame\n",
    "    attribs: Columns specified to be transformed. Expected data structure is a list\n",
    "    \n",
    "    returns: X(Sparse Matrix): y(Series)\n",
    "    \"\"\"\n",
    "    cat_encoder = OneHotEncoder()\n",
    "    X = cat_encoder.fit_transform(df[attribs])\n",
    "    \n",
    "    y = (df[\"arrest\"] == True).astype(np.int)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29fb849",
   "metadata": {},
   "source": [
    "# Building the Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ff4014",
   "metadata": {},
   "source": [
    "We'll only consider traditional models in Part 1. Part 2 will specifically use deep learning techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57c3775",
   "metadata": {},
   "source": [
    "Now that we've prepared the data, we can build the models and get a baseline accuracy and f1-score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b18f089",
   "metadata": {},
   "source": [
    "Since the data is so large, we'll only consider a small random subset to fit different models quickly. Note that it is important to stratify on arrests since we have strong class imbalance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66aeeb55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 14.1 s\n"
     ]
    }
   ],
   "source": [
    "#List of variables to use in the model\n",
    "cat_attribs = [\"primary_type\", \"location_description\", \"domestic\", \"district_name\", \"community_name\", \"Month\", \"Hour\"]\n",
    "\n",
    "#Prepare the data for modelling\n",
    "X, y = prepare_chicago_train(df = chicago_train, attribs = cat_attribs.copy())\n",
    "\n",
    "#Subset the data twice to quickly fit models\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size =.90, random_state = 42, stratify = y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size =.20, random_state = 42, stratify = y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d28a8064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(225170, 181)\n",
      "(56293, 181)\n",
      "(225170,)\n",
      "(56293,)\n"
     ]
    }
   ],
   "source": [
    "#Check the shapes\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdeb741b",
   "metadata": {},
   "source": [
    "# Baseline Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb68bd33",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2278eaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(max_iter = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "591a98af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression 5-fold CV Baseline F1-Score: 65.23%\n",
      "Logistic Regression 5-fold CV Baseline Accuracy: 86.62%\n",
      "Wall time: 19.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_train_pred = cross_val_predict(log_reg, X_train, y_train, cv = 5)\n",
    "lr_cv_f1 = np.round(f1_score(y_train, y_train_pred), 4) * 100\n",
    "lr_cv_acc = np.round(accuracy_score(y_train, y_train_pred), 4) * 100\n",
    "print(f'Logistic Regression 5-fold CV Baseline F1-Score: {lr_cv_f1:.2f}%')\n",
    "print(f'Logistic Regression 5-fold CV Baseline Accuracy: {lr_cv_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f80924b",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "426441f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_clf = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3e091408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes 5-fold CV Baseline F1-Score: 61.65%\n",
      "Naive Bayes 5-fold CV Baseline Accuracy: 83.24%\n",
      "Wall time: 6.28 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Naive Bayes API expects arrays to be passed\n",
    "y_train_pred = cross_val_predict(nb_clf, X_train.toarray(), np.array(y_train), cv = 5)\n",
    "nb_cv_f1 = np.round(f1_score(y_train, y_train_pred), 4) * 100\n",
    "nb_cv_acc = np.round(accuracy_score(y_train, y_train_pred), 4) * 100\n",
    "print(f'Naive Bayes 5-fold CV Baseline F1-Score: {nb_cv_f1:.2f}%')\n",
    "print(f'Naive Bayes 5-fold CV Baseline Accuracy: {nb_cv_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a40fda",
   "metadata": {},
   "source": [
    "### Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "99ccbb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_clf = LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c5a802ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVC 5-fold CV Baseline F1-Score: 65.62%\n",
      "Linear SVC 5-fold CV Baseline Accuracy: 86.95%\n",
      "Wall time: 56.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_train_pred = cross_val_predict(svc_clf, X_train, y_train, cv = 5)\n",
    "svc_cv_f1 = np.round(f1_score(y_train, y_train_pred), 4) * 100\n",
    "svc_cv_acc = np.round(accuracy_score(y_train, y_train_pred), 4) * 100\n",
    "print(f'Linear SVC 5-fold CV Baseline F1-Score: {svc_cv_f1:.2f}%')\n",
    "print(f'Linear SVC 5-fold CV Baseline Accuracy: {svc_cv_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124ba58d",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c0c330c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(n_estimators = 50, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ea4b0f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest 3-fold CV Baseline F1-Score: 67.147%\n",
      "Random Forest 3-fold CV Baseline Accuracy: 86.321%\n",
      "Wall time: 7min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_train_pred = cross_val_predict(rf_clf, X_train, y_train, cv = 3)\n",
    "rf_cv_f1 = np.round(f1_score(y_train, y_train_pred), 5) * 100\n",
    "rf_cv_acc = np.round(accuracy_score(y_train, y_train_pred), 5) * 100\n",
    "print(f'Random Forest 3-fold CV Baseline F1-Score: {rf_cv_f1}%')\n",
    "print(f'Random Forest 3-fold CV Baseline Accuracy: {rf_cv_acc}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "decc97bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf = RandomForestClassifier(n_estimators = 50, max_depth=25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e65db2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest 3-fold CV Baseline F1-Score: 66.43%\n",
      "Random Forest 3-fold CV Baseline Accuracy: 87.45%\n",
      "Wall time: 1min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_train_pred = cross_val_predict(rf_clf, X_train, y_train, cv = 3)\n",
    "rf_cv_f1 = np.round(f1_score(y_train, y_train_pred), 5) * 100\n",
    "rf_cv_acc = np.round(accuracy_score(y_train, y_train_pred), 5) * 100\n",
    "print(f'Random Forest 3-fold CV Baseline F1-Score: {rf_cv_f1:.2f}%')\n",
    "print(f'Random Forest 3-fold CV Baseline Accuracy: {rf_cv_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05da09d0",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "206246fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf = xgboost.XGBClassifier(use_label_encoder=False, objective = \"binary:logistic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bc45a247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:15:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:15:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:15:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:16:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:16:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Wall time: 17.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_train_pred = cross_val_predict(xgb_clf, X_train, y_train, cv = 5)\n",
    "xgb_cv_f1 = np.round(f1_score(y_train, y_train_pred), 4) * 100\n",
    "xgb_cv_acc = np.round(accuracy_score(y_train, y_train_pred), 4) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5a080387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost 5-fold CV Baseline F1-Score: 68.47%\n",
      "XGBoost 5-fold CV Baseline Accuracy: 87.75%\n"
     ]
    }
   ],
   "source": [
    "print(f'XGBoost 5-fold CV Baseline F1-Score: {xgb_cv_f1:.2f}%')\n",
    "print(f'XGBoost 5-fold CV Baseline Accuracy: {xgb_cv_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63bf723",
   "metadata": {},
   "source": [
    "### Voting Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9fb5f9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf = LogisticRegression(max_iter = 10000)\n",
    "nb_clf = GaussianNB()\n",
    "svc_clf = LinearSVC()\n",
    "rf_clf = RandomForestClassifier(n_estimators=50, max_depth=25, random_state=42)\n",
    "xgb_clf = xgboost.XGBClassifier(use_label_encoder=False, objective = \"binary:logistic\")\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators = [(\"lr\", lr_clf), (\"nb\", nb_clf), (\"svc\", svc_clf), (\"rf\", rf_clf), (\"xgb\", xgb_clf)],\n",
    "    voting = \"hard\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "13bfaa21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:24:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:25:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[10:26:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Wall time: 4min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_train_pred = cross_val_predict(voting_clf, X_train.toarray(), np.array(y_train), cv = 3)\n",
    "vt_cv_f1 = np.round(f1_score(y_train, y_train_pred), 4) * 100\n",
    "vt_cv_acc = np.round(accuracy_score(y_train, y_train_pred), 4) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e564d40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Ensemble 3-fold CV Baseline F1-Score: 65.95%\n",
      "Voting Ensemble 3-fold CV Baseline Accuracy: 87.14%\n"
     ]
    }
   ],
   "source": [
    "print(f'Voting Ensemble 3-fold CV Baseline F1-Score: {vt_cv_f1:.2f}%')\n",
    "print(f'Voting Ensemble 3-fold CV Baseline Accuracy: {vt_cv_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd424c7",
   "metadata": {},
   "source": [
    "### Preliminary Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec511e5",
   "metadata": {},
   "source": [
    "All six models give similar accuracy scores of 87% with Naive Bayes being a bit lower at 83%. However, the models have more variation in their F1-score. All but the Naive Bayes model have F1-scores around 66% but Naive Bayes only has an F1-score of 61%. We can also see that the Random Forest model takes the most amount of time to fit when the max depth parameter is not specified. Therefore, we refit with the max depth specified as 25. This led to a slight reduction in the F1-score and a slight increase in accuracy but overall the model fit much faster than before. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e67b38",
   "metadata": {},
   "source": [
    "# Fine Tune the System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0ed974",
   "metadata": {},
   "source": [
    "Now that we have some preliminary results, we can go ahead and fine tune the hyparameters. Naive Bayes does not have any hyperparameters that need to be tuned. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa6cac5",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c9a9d648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 5.191519151915192}\n",
      "0.6522769550687257\n",
      "Wall time: 1min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "param_distribs = {\n",
    "    \"C\" : np.linspace(0, 10, 10000),\n",
    "    }\n",
    "lr_clf = LogisticRegression(penalty = \"l2\", solver = \"lbfgs\", max_iter=100000)\n",
    "\n",
    "lr_rnd_search_cv = RandomizedSearchCV(lr_clf, param_distribs, n_iter = 10,\n",
    "                                   cv=3 ,scoring = 'f1', random_state=42, n_jobs = -1)\n",
    "\n",
    "lr_rnd_search_cv.fit(X_train, y_train)\n",
    "\n",
    "print(lr_rnd_search_cv.best_params_)\n",
    "print(lr_rnd_search_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "41c89e34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_C</th>\n",
       "      <th>mean_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.191519</td>\n",
       "      <td>0.652277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.265627</td>\n",
       "      <td>0.652277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.270727</td>\n",
       "      <td>0.652269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.390539</td>\n",
       "      <td>0.652269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4.426443</td>\n",
       "      <td>0.652269</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    param_C  mean_test_score\n",
       "3  5.191519         0.652277\n",
       "5  6.265627         0.652277\n",
       "0  7.270727         0.652269\n",
       "2  5.390539         0.652269\n",
       "7  4.426443         0.652269"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Save the results in a dataframe\n",
    "lr_rnd_search_df = pd.DataFrame(lr_rnd_search_cv.cv_results_)\n",
    "#Rank the results by score\n",
    "lr_rnd_search_df[[\"param_C\", \"mean_test_score\"]].sort_values(by = \"mean_test_score\", ascending = False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db891e8",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning for Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7fd6c384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.1}\n",
      "0.6562517192454032\n",
      "Wall time: 16min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "param_distribs = {\n",
    "    \"C\" : np.linspace(0.1, 15, 100),\n",
    "    }\n",
    "svc_clf = LinearSVC(max_iter=10000)\n",
    "\n",
    "svc_rnd_search_cv = RandomizedSearchCV(svc_clf, param_distribs, n_iter = 10,\n",
    "                                   cv=3 ,scoring = 'f1', random_state=42, n_jobs = -1)\n",
    "\n",
    "svc_rnd_search_cv.fit(X_train, y_train)\n",
    "\n",
    "print(svc_rnd_search_cv.best_params_)\n",
    "print(svc_rnd_search_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "37acd4bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_C</th>\n",
       "      <th>mean_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.656252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.591919</td>\n",
       "      <td>0.656244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.076768</td>\n",
       "      <td>0.656244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10.635354</td>\n",
       "      <td>0.656244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.872727</td>\n",
       "      <td>0.656244</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     param_C  mean_test_score\n",
       "9        0.1         0.656252\n",
       "0  12.591919         0.656244\n",
       "1   8.076768         0.656244\n",
       "2  10.635354         0.656244\n",
       "3   6.872727         0.656244"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc_rnd_search_df = pd.DataFrame(svc_rnd_search_cv.cv_results_)\n",
    "svc_rnd_search_df[[\"param_C\", \"mean_test_score\"]].sort_values(by = \"mean_test_score\", ascending = False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea1efca",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning for RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2e7208a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 69, 'max_depth': 27}\n",
      "0.6663755472920951\n",
      "Wall time: 9min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "param_distribs = {\n",
    "    \"n_estimators\": np.arange(25, 150),\n",
    "    \"max_depth\": np.arange(10, 30)\n",
    "    }\n",
    "rf_clf = RandomForestClassifier()\n",
    "\n",
    "rf_rnd_search_cv = RandomizedSearchCV(rf_clf, param_distribs, n_iter = 10,\n",
    "                                   cv=2 ,scoring = 'f1', random_state=42)\n",
    "\n",
    "rf_rnd_search_cv.fit(X_train, y_train)\n",
    "\n",
    "print(rf_rnd_search_cv.best_params_)\n",
    "print(rf_rnd_search_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "89b93bbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>mean_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>69</td>\n",
       "      <td>27</td>\n",
       "      <td>0.666376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38</td>\n",
       "      <td>23</td>\n",
       "      <td>0.657190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>132</td>\n",
       "      <td>21</td>\n",
       "      <td>0.655763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69</td>\n",
       "      <td>20</td>\n",
       "      <td>0.654347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>138</td>\n",
       "      <td>19</td>\n",
       "      <td>0.652988</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  param_n_estimators param_max_depth  mean_test_score\n",
       "5                 69              27         0.666376\n",
       "4                 38              23         0.657190\n",
       "9                132              21         0.655763\n",
       "1                 69              20         0.654347\n",
       "7                138              19         0.652988"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_rnd_search_df = pd.DataFrame(rf_rnd_search_cv.cv_results_)\n",
    "tuned_params = [\"param_n_estimators\", \"param_max_depth\", \"mean_test_score\"]\n",
    "rf_rnd_search_df[tuned_params].sort_values(by = \"mean_test_score\", ascending = False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d536e43",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning for XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "abc229ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distribs = {\n",
    "    \"max_depth\": [2,3,4,5,6,7],\n",
    "    \"gamma\": uniform(loc = 0.0, scale = 3),\n",
    "    \"min_child_weight\": list(range(20,51)),\n",
    "    \"colsample_bytree\": uniform(loc = 0.1, scale = 0.9),\n",
    "    \"learning_rate\": uniform(loc = 0.01, scale = 0.5),\n",
    "    \"subsample\": uniform(loc = 0.5, scale = 0.5),\n",
    "    \"reg_lambda\": uniform(loc = 0.01, scale = 3)\n",
    "    }\n",
    "rng = np.random.RandomState(42)\n",
    "n_iter = 50\n",
    "param_list = list(ParameterSampler(param_distribs, n_iter = n_iter, random_state=rng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "732cf524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with 5 of 50\n",
      "Done with 10 of 50\n",
      "Done with 15 of 50\n",
      "Done with 20 of 50\n",
      "Done with 25 of 50\n",
      "Done with 30 of 50\n",
      "Done with 35 of 50\n",
      "Done with 40 of 50\n",
      "Done with 45 of 50\n",
      "Done with 50 of 50\n",
      "Wall time: 57min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "eval_set = [(X_train, y_train), (X_val, y_val)]\n",
    "val_f1_score = []\n",
    "n_est = []\n",
    "counter = 1\n",
    "xgb_cf = xbg_clf = xgboost.XGBClassifier(n_estimators = 1000, use_label_encoder=False, objective = \"binary:logistic\")\n",
    "\n",
    "for params in param_list:\n",
    "    xgb_cf.set_params(**params)\n",
    "    xgb_cf.fit(X_train, y_train, eval_set=eval_set, eval_metric = \"auc\", verbose = False, early_stopping_rounds = 30)\n",
    "    val_set_preds = xgb_cf.predict(X_val)\n",
    "    val_f1_score.append(f1_score(y_val, val_set_preds))\n",
    "    n_est.append(int(xgb_cf.get_booster().attributes()[\"best_ntree_limit\"]))\n",
    "    if counter % 5 == 0:\n",
    "        print(f'Done with {counter} of {n_iter}')\n",
    "    counter += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1e1a6c4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>gamma</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_child_weight</th>\n",
       "      <th>reg_lambda</th>\n",
       "      <th>subsample</th>\n",
       "      <th>Validation F1-Score</th>\n",
       "      <th>N Estimators</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.731770</td>\n",
       "      <td>2.387378</td>\n",
       "      <td>0.455003</td>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "      <td>0.291946</td>\n",
       "      <td>0.789140</td>\n",
       "      <td>0.690578</td>\n",
       "      <td>173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.451955</td>\n",
       "      <td>0.546708</td>\n",
       "      <td>0.387681</td>\n",
       "      <td>7</td>\n",
       "      <td>25</td>\n",
       "      <td>0.633825</td>\n",
       "      <td>0.783850</td>\n",
       "      <td>0.690087</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.225398</td>\n",
       "      <td>1.813252</td>\n",
       "      <td>0.279921</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>2.838561</td>\n",
       "      <td>0.799433</td>\n",
       "      <td>0.689916</td>\n",
       "      <td>253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.557933</td>\n",
       "      <td>1.908998</td>\n",
       "      <td>0.135231</td>\n",
       "      <td>7</td>\n",
       "      <td>25</td>\n",
       "      <td>2.946679</td>\n",
       "      <td>0.743371</td>\n",
       "      <td>0.689536</td>\n",
       "      <td>447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.393887</td>\n",
       "      <td>1.711332</td>\n",
       "      <td>0.270417</td>\n",
       "      <td>7</td>\n",
       "      <td>27</td>\n",
       "      <td>2.543602</td>\n",
       "      <td>0.873660</td>\n",
       "      <td>0.689530</td>\n",
       "      <td>204</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    colsample_bytree     gamma  learning_rate  max_depth  min_child_weight  \\\n",
       "45          0.731770  2.387378       0.455003          5                21   \n",
       "7           0.451955  0.546708       0.387681          7                25   \n",
       "17          0.225398  1.813252       0.279921          6                23   \n",
       "36          0.557933  1.908998       0.135231          7                25   \n",
       "9           0.393887  1.711332       0.270417          7                27   \n",
       "\n",
       "    reg_lambda  subsample  Validation F1-Score  N Estimators  \n",
       "45    0.291946   0.789140             0.690578           173  \n",
       "7     0.633825   0.783850             0.690087           140  \n",
       "17    2.838561   0.799433             0.689916           253  \n",
       "36    2.946679   0.743371             0.689536           447  \n",
       "9     2.543602   0.873660             0.689530           204  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_param_search_df = pd.DataFrame(param_list)\n",
    "xgb_param_search_df[\"Validation F1-Score\"] = val_f1_score\n",
    "xgb_param_search_df[\"N Estimators\"] = n_est\n",
    "xgb_param_search_df.sort_values(by=\"Validation F1-Score\", ascending = False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8b71f3",
   "metadata": {},
   "source": [
    "# Validation Scores on Full Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d8679750",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_attribs = [\"primary_type\", \"location_description\", \"domestic\", \"district_name\", \"community_name\", \"Month\", \"Hour\"]\n",
    "X, y = prepare_chicago_train(df = chicago_train, attribs = cat_attribs.copy())\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size =.10, random_state = 42, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d8b9e71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2533172, 181)\n",
      "(281464, 181)\n",
      "(2533172,)\n",
      "(281464,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903c1368",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "69038c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lr_f1 = lr_rnd_search_df[\"mean_test_score\"].argmax()\n",
    "best_C = lr_rnd_search_df.loc[max_lr_f1, \"param_C\"]\n",
    "\n",
    "log_reg = LogisticRegression(penalty = \"l2\", C = best_C, solver = \"lbfgs\", random_state=42, max_iter = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7cfd1fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(log_reg, open(\"log_reg_model.sav\",'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4f04619d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 57s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=5.191519151915192, max_iter=10000, random_state=42)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "log_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2c69fa7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Validation F1-Score: 65.07%\n",
      "Logistic Regression Validation Accuracy Score: 86.59%\n"
     ]
    }
   ],
   "source": [
    "y_pred = log_reg.predict(X_val)\n",
    "lr_val_f1 = np.round(f1_score(y_pred, y_val), 5) * 100\n",
    "lr_val_acc = np.round(accuracy_score(y_pred, y_val), 5) * 100\n",
    "print(f'Logistic Regression Validation F1-Score: {lr_val_f1:.2f}%')\n",
    "print(f'Logistic Regression Validation Accuracy Score: {lr_val_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0973037e",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fd9ee2ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_clf = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "bd71996e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 15s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "nb_clf.fit(X_train.toarray(), np.array(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c4e29332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Validation F1-Score: 62.51%\n",
      "Naive Bayes Validation Accuracy Score: 84.44%\n"
     ]
    }
   ],
   "source": [
    "y_pred = nb_clf.predict(X_val.toarray())\n",
    "nb_val_f1 = np.round(f1_score(y_pred, y_val), 5) * 100\n",
    "nb_val_acc = np.round(accuracy_score(y_pred, y_val), 5) * 100\n",
    "print(f'Naive Bayes Validation F1-Score: {nb_val_f1:.2f}%')\n",
    "print(f'Naive Bayes Validation Accuracy Score: {nb_val_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188ae013",
   "metadata": {},
   "source": [
    "### Linear SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c853a94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_svc_f1 = svc_rnd_search_df[\"mean_test_score\"].argmax()\n",
    "best_C = svc_rnd_search_df.loc[max_svc_f1, \"param_C\"]\n",
    "\n",
    "svc_clf = LinearSVC(penalty=\"l2\", C = best_C, max_iter=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "68413e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(svc_clf, open(\"svc_clf_model.sav\",'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "99555ee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 25.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=0.1, max_iter=10000)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "svc_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d7e76deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear SVC Validation F1-Score: 65.44%\n",
      "Linear SVC Validation Accuracy Score: 86.92%\n"
     ]
    }
   ],
   "source": [
    "y_pred = svc_clf.predict(X_val)\n",
    "svc_val_f1 = np.round(f1_score(y_pred, y_val), 5) * 100\n",
    "svc_val_acc = np.round(accuracy_score(y_pred, y_val), 5) * 100\n",
    "print(f'Linear SVC Validation F1-Score: {svc_val_f1:.2f}%')\n",
    "print(f'Linear SVC Validation Accuracy Score: {svc_val_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d6eba4",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "71b6d4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_rf_f1 = rf_rnd_search_df[\"mean_test_score\"].argmax()\n",
    "best_n_est = rf_rnd_search_df.loc[max_rf_f1, \"param_n_estimators\"]\n",
    "best_max_depth = rf_rnd_search_df.loc[max_rf_f1, \"param_max_depth\"]\n",
    "\n",
    "rf_clf = RandomForestClassifier(n_estimators=best_n_est, max_depth=best_max_depth, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8af42919",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(rf_clf, open(\"rf_clf_model.sav\",'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "43139712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1h 6min 18s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=27, n_estimators=69, random_state=42)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "rf_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "fec3c8ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Validation F1-Score: 66.64%\n",
      "Random Forest Validation Accuracy Score: 87.53%\n"
     ]
    }
   ],
   "source": [
    "y_pred = rf_clf.predict(X_val)\n",
    "rf_val_f1 = np.round(f1_score(y_pred, y_val), 5) * 100\n",
    "rf_val_acc = np.round(accuracy_score(y_pred, y_val), 5) * 100\n",
    "print(f'Random Forest Validation F1-Score: {rf_val_f1:.2f}%')\n",
    "print(f'Random Forest Validation Accuracy Score: {rf_val_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfacfbe",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "887bd48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_xgb_f1 = xgb_param_search_df[\"Validation F1-Score\"].argmax()\n",
    "best_colsample = xgb_param_search_df.loc[max_xgb_f1, \"colsample_bytree\"]\n",
    "best_gamma = xgb_param_search_df.loc[max_xgb_f1, \"gamma\"]\n",
    "best_lr = xgb_param_search_df.loc[max_xgb_f1, \"learning_rate\"]\n",
    "best_max_depth = xgb_param_search_df.loc[max_xgb_f1, \"max_depth\"]\n",
    "best_min_child = xgb_param_search_df.loc[max_xgb_f1, \"min_child_weight\"]\n",
    "best_reg_lambda = xgb_param_search_df.loc[max_xgb_f1, \"reg_lambda\"]\n",
    "best_subsample = xgb_param_search_df.loc[max_xgb_f1, \"subsample\"]\n",
    "best_n_est = xgb_param_search_df.loc[max_xgb_f1, \"N Estimators\"]\n",
    "\n",
    "xgb_clf = xgboost.XGBClassifier(n_estimators=best_n_est, colsample_bytree = best_colsample, gamma = best_gamma, \n",
    "                                learning_rate=best_lr, max_depth = best_max_depth, min_child_weight=best_min_child,\n",
    "                                reg_lambda=best_reg_lambda, subsample=best_subsample,\n",
    "                                use_label_encoder=False, objective = \"binary:logistic\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ae8f4bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(xgb_clf, open(\"xgb_clf_model.sav\",'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fbc64984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13:13:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "Wall time: 2min 18s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.731770189531933,\n",
       "              enable_categorical=False, gamma=2.3873780083083034, gpu_id=-1,\n",
       "              importance_type=None, interaction_constraints='',\n",
       "              learning_rate=0.45500267090878316, max_delta_step=0, max_depth=5,\n",
       "              min_child_weight=21, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=173, n_jobs=8, num_parallel_tree=1, predictor='auto',\n",
       "              random_state=0, reg_alpha=0, reg_lambda=0.291945819522607,\n",
       "              scale_pos_weight=1, subsample=0.789140070498087,\n",
       "              tree_method='exact', use_label_encoder=False,\n",
       "              validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "xgb_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "65481cd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Validation F1-Score: 69.00%\n",
      "XGBoost Forest Validation Accuracy Score: 87.89%\n"
     ]
    }
   ],
   "source": [
    "y_pred = xgb_clf.predict(X_val)\n",
    "xgb_val_f1 = np.round(f1_score(y_pred, y_val), 5) * 100\n",
    "xgb_val_acc = np.round(accuracy_score(y_pred, y_val), 5) * 100\n",
    "print(f'XGBoost Validation F1-Score: {xgb_val_f1:.2f}%')\n",
    "print(f'XGBoost Forest Validation Accuracy Score: {xgb_val_acc:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1574ea7",
   "metadata": {},
   "source": [
    "### Voting Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "3d4fa54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_clf = VotingClassifier(\n",
    "    estimators = [(\"lr\", log_reg), (\"nb\", nb_clf), (\"svc\", svc_clf), (\"rf\", rf_clf), (\"xgb\", xgb_clf)],\n",
    "    voting = \"hard\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c6274132",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(voting_clf, open(\"voting_clf_model.sav\",'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "103b4aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:35:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr',\n",
       "                              LogisticRegression(C=2.4332433243324334,\n",
       "                                                 max_iter=10000,\n",
       "                                                 random_state=42)),\n",
       "                             ('rf',\n",
       "                              RandomForestClassifier(max_depth=14,\n",
       "                                                     n_estimators=255,\n",
       "                                                     n_jobs=-1,\n",
       "                                                     random_state=42)),\n",
       "                             ('xgb',\n",
       "                              XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                                            colsample_bylevel=1,\n",
       "                                            colsample_bynode=1,\n",
       "                                            colsample_bytree=0.7273141668957411,\n",
       "                                            enable_categorical=False,\n",
       "                                            ga...\n",
       "                                            learning_rate=0.1897455756098776,\n",
       "                                            max_delta_step=0, max_depth=6,\n",
       "                                            min_child_weight=29, missing=nan,\n",
       "                                            monotone_constraints='()',\n",
       "                                            n_estimators=422, n_jobs=8,\n",
       "                                            num_parallel_tree=1,\n",
       "                                            predictor='auto', random_state=0,\n",
       "                                            reg_alpha=0,\n",
       "                                            reg_lambda=2.4380834664355406,\n",
       "                                            scale_pos_weight=1,\n",
       "                                            subsample=0.9050566973395904,\n",
       "                                            tree_method='exact',\n",
       "                                            use_label_encoder=False,\n",
       "                                            validate_parameters=1,\n",
       "                                            verbosity=None))])"
      ]
     },
     "execution_count": 476,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voting_clf.fit(X_train.toarray(), np.array(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "5681d7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Classifier Validation F1-Score: 0.6495456833208312\n",
      "Voting Validation Accuracy Score: 0.8710811173572511\n"
     ]
    }
   ],
   "source": [
    "y_pred = voting_clf.predict(X_val)\n",
    "vt_val_f1 = np.round(f1_score(y_pred, y_val), 5) * 100\n",
    "vt_val_acc = np.round(accuracy_score(y_pred, y_val), 5) * 100\n",
    "print(f'Voting Classifier Validation F1-Score: {vt_val_f1:.2f}%')\n",
    "print(f'Voting Classifier Validation Accuracy Score: {vt_val_acc:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
